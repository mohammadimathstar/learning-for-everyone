{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1d526dd-0306-481d-ba07-b5a63628a24e",
   "metadata": {},
   "source": [
    "### Web Scraping Tutorial: Scraping Podcasts from eenbeetjenederlands.nl\n",
    "\n",
    "---\n",
    "\n",
    "# Introduction\n",
    "\n",
    "## What is Web Scraping?\n",
    "Web scraping is the process of extracting data from websites. This tutorial will guide you through building a simple web scraper to collect podcast episode data and transcripts from the website [eenbeetjenederlands.nl](https://www.eenbeetjenederlands.nl/).\n",
    "\n",
    "---\n",
    "\n",
    "## Overview of the Steps\n",
    "In this tutorial, we will:\n",
    "1. **Import the Required Libraries** – Set up the necessary Python packages.\n",
    "2. **Configure Logging** – Track the scraper's progress and log errors.\n",
    "3. **Initialize Variables** – Define the base URL and data storage structures.\n",
    "4. **Scrape Episode Information** – Loop through web pages to extract episode data.\n",
    "5. **Save Data to CSV** – Store the collected data in a CSV file.\n",
    "6. **Scrape Transcripts** – Visit each episode page and extract the transcript.\n",
    "7. **Download Audio Previews** – Retrieve and save audio previews for each episode.\n",
    "\n",
    "---\n",
    "\n",
    "# Setup and Prerequisites\n",
    "\n",
    "Before we begin, ensure you have the following libraries installed:\n",
    "```bash\n",
    "pip install requests beautifulsoup4 pandas tqdm\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Step 1: Import Required Libraries\n",
    "\n",
    "### Description:\n",
    "- **requests**: For making HTTP requests to fetch webpage content. \n",
    "  - `requests.get(url)`: Sends a GET request to the specified URL and retrieves the page's content.\n",
    "  - `response.raise_for_status()`: Raises an exception if the request returns an HTTP error status.\n",
    "- **BeautifulSoup**: For parsing HTML and extracting data from the webpage.\n",
    "  - `BeautifulSoup(response.content, 'html.parser')`: Parses the HTML content of the webpage.\n",
    "  - `soup.find()`: Finds the first matching HTML element.\n",
    "  - `soup.find_all()`: Finds all matching elements.\n",
    "- **pandas**: For handling and saving data.\n",
    "  - `pd.DataFrame()`: Creates a structured DataFrame from a list or dictionary.\n",
    "  - `df.to_csv()`: Saves the DataFrame to a CSV file.\n",
    "- **logging**: For logging errors and progress.\n",
    "  - `logging.info()`: Logs informational messages.\n",
    "  - `logging.error()`: Logs error messages.\n",
    "- **tqdm**: For creating progress bars during iteration.\n",
    "  - `tqdm.tqdm()`: Wraps an iterable to show progress during loops.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35f729f4-33fc-4043-9376-634fed1490e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f27e5b-360a-4568-b51e-b755398d23a8",
   "metadata": {},
   "source": [
    "# Step 2: Setup Logging\n",
    "\n",
    "### Description:\n",
    "- Configures a logging system to track errors and information throughout the scraping process.\n",
    "- Logs are saved to `eenbeetjenederlands.log` for debugging and tracking purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "575a746a-9906-4ae7-aa88-8b3281cc248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    filename='eenbeetjenederlands.log', \n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709ed78c-130c-45e1-942e-dd8ad230c512",
   "metadata": {},
   "source": [
    "# Step 3: Initialize Variables\n",
    "\n",
    "### Description:\n",
    "- Initializes the starting URL for the scraper and an empty list to store episode data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b4f2511-0d24-42d3-90c0-6292c43d68e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.eenbeetjenederlands.nl/'\n",
    "episodes_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b65f5-7c85-4768-9fa6-cdccab6ac99a",
   "metadata": {},
   "source": [
    "# Step 4: Scraping Episodes List\n",
    "\n",
    "### Description:\n",
    "- Loops through each page of the podcast site.\n",
    "- Extracts episode titles, summaries, and links, appending them to a list.\n",
    "- Stops when no next page is found.\n",
    "- A short delay is added between requests to prevent overwhelming the server.\n",
    "\n",
    "__Methods:__\n",
    "\n",
    "- **requests.get(base_url)**: Sends a GET request to fetch the webpage content.\n",
    "- **response.raise_for_status()**: Ensures the response is successful; logs error if the request fails.\n",
    "- **soup.find()**: Locates the first HTML `tag` containing episode articles.\n",
    "- **soup.find_all()**: Collects all elements with the given `tags`.\n",
    "- **sleep(1)**: Adds a delay between requests to prevent overloading the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b536230-d75e-4fe2-8006-9b3a959d56e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while base_url:\n",
    "    try:\n",
    "        response = requests.get(base_url)\n",
    "        response.raise_for_status()  # Raise error if request fails\n",
    "    except requests.exceptions.HTTPError as error:\n",
    "        logger.error(error)\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    episodes = soup.find('div', id='afleveringen').find_all('article')\n",
    "    for episode in episodes:\n",
    "        episode_link = episode.find('a', class_='button')['href']\n",
    "        episode_title = episode.find('h2').text\n",
    "        episode_summary = episode.find('p').text\n",
    "\n",
    "        episodes_list.append({'title': episode_title, 'summary': episode_summary, 'link': episode_link})\n",
    "    \n",
    "    next_page_link = soup.find('a', class_='next page-numbers')\n",
    "    base_url = next_page_link['href'] if next_page_link else None\n",
    "    \n",
    "    time.sleep(1)  # Avoid overwhelming the server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f41c19-70df-4bf2-8b75-a99645ceb5c7",
   "metadata": {},
   "source": [
    "# Step 5: Save Episodes Data to CSV\n",
    "\n",
    "### Description:\n",
    "- Converts the episode data list to a DataFrame and saves it to a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fab337d-46df-4d32-b924-45f242631374",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_df = pd.DataFrame(data=episodes_list)\n",
    "episodes_df.to_csv('eenbeetjenederlands.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3924d-eecb-440d-99b3-5df1f8922d1e",
   "metadata": {},
   "source": [
    "# Step 6: Scraping Transcripts and Audio Files\n",
    "\n",
    "### Description:\n",
    "- Iterates over each episode link, scrapes the transcript, and logs a warning if none is found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b9e46a2-c427-4cf6-b4d0-f3087b512563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 74/74 [00:54<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('eenbeetjenederlands.csv')\n",
    "episode_links = df['link'].values\n",
    "\n",
    "transcripts = []\n",
    "for i, episode_url in tqdm.tqdm(enumerate(episode_links), total=len(episode_links)):\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(episode_url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        logger.error(err)\n",
    "        continue\n",
    "\n",
    "    title = df.loc[df['link'] == episode_url, 'title'].iloc[0].replace('/', '_')\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    text_finder = soup.find('div', class_='wp-block-group__inner-container is-layout-flow wp-block-group-is-layout-flow')\n",
    "    \n",
    "    if text_finder:\n",
    "        transcript = \"\\n\\n\".join([p.text for p in text_finder.find_all('p')])\n",
    "    else:\n",
    "        transcript = \"\"\n",
    "        logger.warning(f\"Transcript not found for episode {title}\")\n",
    "    transcripts.append(transcript)\n",
    "\n",
    "    # step 7. Download Audio Previews\n",
    "    player_iframe = soup.find('iframe', class_='player')\n",
    "    if player_iframe:\n",
    "        player_url = player_iframe['src']\n",
    "        player_response = requests.get(player_url)\n",
    "        player_soup = BeautifulSoup(player_response.content, 'html.parser')\n",
    "    \n",
    "        next_data_script = player_soup.find('script', id=\"__NEXT_DATA__\")\n",
    "        next_data = json.loads(next_data_script.text)\n",
    "    \n",
    "        audio_preview_url = next_data['props']['pageProps']['state']['data']['entity']['audioPreview']['url']\n",
    "    \n",
    "        audio_response = requests.get(audio_preview_url)\n",
    "        clip_dir = './data/clips/'\n",
    "        \n",
    "        if not os.path.exists(clip_dir):\n",
    "            os.makedirs(clip_dir)\n",
    "    \n",
    "        with open(clip_dir + title + \".mp3\", \"wb\") as file:\n",
    "            file.write(audio_response.content)\n",
    "    else:\n",
    "        logger.warning(f\"No player found for episode {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b50811f0-3dd6-4c9e-8374-3356231e2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['transcript'] = transcripts\n",
    "df.to_csv('eenbeetjenederlands.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5858b21-5172-402c-b84f-211ea5a94a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5566c503-6948-478c-94bf-2c3618b7aea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>link</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#70: Natuur in Nederland</td>\n",
       "      <td>Ook al is Nederland een klein en dichtbevolkt ...</td>\n",
       "      <td>https://www.eenbeetjenederlands.nl/podcast/nat...</td>\n",
       "      <td>Hallo allemaal! Dit is Een Beetje Nederlands, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#69: Anna Maria van Schurman</td>\n",
       "      <td>Veel mensen denken dat Aletta Jacobs de eerste...</td>\n",
       "      <td>https://www.eenbeetjenederlands.nl/podcast/ann...</td>\n",
       "      <td>Hallo allemaal! Dit is Een Beetje Nederlands, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#68: Tachtigjarige Oorlog (deel 2)</td>\n",
       "      <td>Het jaar 1568 is een van de belangrijkste jaar...</td>\n",
       "      <td>https://www.eenbeetjenederlands.nl/podcast/68-...</td>\n",
       "      <td>Hallo allemaal! Dit is Een Beetje Nederlands, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#67: Tachtigjarige Oorlog (deel 1)</td>\n",
       "      <td>Het jaar 1568 is een van de belangrijkste jaar...</td>\n",
       "      <td>https://www.eenbeetjenederlands.nl/podcast/67-...</td>\n",
       "      <td>Hallo allemaal! Dit is Een Beetje Nederlands, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#66: Zwangerschap in Nederland</td>\n",
       "      <td>In deze aflevering: hoe gaat een zwangerschap ...</td>\n",
       "      <td>https://www.eenbeetjenederlands.nl/podcast/zwa...</td>\n",
       "      <td>Hallo allemaal! Dit is Een Beetje Nederlands, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title  \\\n",
       "0            #70: Natuur in Nederland   \n",
       "1        #69: Anna Maria van Schurman   \n",
       "2  #68: Tachtigjarige Oorlog (deel 2)   \n",
       "3  #67: Tachtigjarige Oorlog (deel 1)   \n",
       "4      #66: Zwangerschap in Nederland   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Ook al is Nederland een klein en dichtbevolkt ...   \n",
       "1  Veel mensen denken dat Aletta Jacobs de eerste...   \n",
       "2  Het jaar 1568 is een van de belangrijkste jaar...   \n",
       "3  Het jaar 1568 is een van de belangrijkste jaar...   \n",
       "4  In deze aflevering: hoe gaat een zwangerschap ...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.eenbeetjenederlands.nl/podcast/nat...   \n",
       "1  https://www.eenbeetjenederlands.nl/podcast/ann...   \n",
       "2  https://www.eenbeetjenederlands.nl/podcast/68-...   \n",
       "3  https://www.eenbeetjenederlands.nl/podcast/67-...   \n",
       "4  https://www.eenbeetjenederlands.nl/podcast/zwa...   \n",
       "\n",
       "                                          transcript  \n",
       "0  Hallo allemaal! Dit is Een Beetje Nederlands, ...  \n",
       "1  Hallo allemaal! Dit is Een Beetje Nederlands, ...  \n",
       "2  Hallo allemaal! Dit is Een Beetje Nederlands, ...  \n",
       "3  Hallo allemaal! Dit is Een Beetje Nederlands, ...  \n",
       "4  Hallo allemaal! Dit is Een Beetje Nederlands, ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d6e7b5-cf3a-4184-b6c5-ed72c4402bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e561394-756c-4a52-8605-ee30e91586da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
