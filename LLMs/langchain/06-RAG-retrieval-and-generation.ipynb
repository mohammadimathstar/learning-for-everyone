{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acd08c0d-d419-432a-ad67-c560dacdd0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45866f4e-f9fa-4180-a34d-f1d1af4690da",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG)\n",
    "\n",
    "In the previous tutorial, we have talked about RAG and its first component indexing. In a simple words, in the indexing step, we create a vector store (or database) which contains the vector/numberical representations of chunks of texts.\n",
    "\n",
    "In this tutorial, we talk about the second component. It consists of:\n",
    "* retrieval: given a query (from a user), we need to find the most relevant information (from the vector store),\n",
    "* generation: given the user's query and the retrieved contexts, the LLM should answer the user.\n",
    "\n",
    "#### step 1) loading the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d8ed55c-afbf-4a7f-9ab0-76bfbafb365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d21817f1-6af7-4081-96a5-f31068ef334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'data/chroma_langchain_db'\n",
    "\n",
    "# the embedding model\n",
    "embedding = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "# To load vector store (containing all vector representations of text's chunks)\n",
    "db = Chroma(\n",
    "    embedding_function=embedding,\n",
    "    persist_directory=persist_directory,  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d9b629-74f6-4d90-af07-f976c36b9d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d97ec54-318c-4b47-83eb-7ea23c9859eb",
   "metadata": {},
   "source": [
    "#### step 2) create a retriever\n",
    "\n",
    "Given a query, a `retriever` can extract relevant chunks from the vector store. Fortunately, langchain provides functionalities that simplify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef204fc3-df62-422d-ae9f-709389847d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GMENT\\n2\\n5.  In September 2013 Judge Ch., sitting in a single-judge formation, \\nbegan the examination of the criminal case.\\n6.  On 30 September 2015 Judge Ch. held a final hearing. '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever()\n",
    "\n",
    "# to test\n",
    "res = retriever.invoke(\"Who are judges?\") # it returns the most 4 relevant text's chunks.\n",
    "res[0].page_content[20:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef98efb-e014-47ce-8691-365cb55bb70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7abfa85-2e96-4a77-81a6-6339e3bc9c60",
   "metadata": {},
   "source": [
    "#### step 3) create a prompt for the LLM\n",
    "\n",
    "To create a prompt which provide both the query (of the user) and the context (i.e. relevant chunks), we can use langchain hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6096604d-ee2c-4129-8713-84e28d036e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub # to download prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bbf41d0-8338-47dd-b8c8-86aa2631a803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/miniconda3/lib/python3.12/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c7de02-e2b5-40a8-bc56-e51c356493ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3f27e32-fa69-4ac1-908d-523a95f1922d",
   "metadata": {},
   "source": [
    "#### step 4) chain\n",
    "\n",
    "Now, we can put all peices together (in a chain) to create a pipeline which \n",
    "* accept a query from the user\n",
    "* then retrieve relevant information from the vector store\n",
    "* and lastly, send the query and the context to the LLM to generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c07fd83-e871-4a34-a14b-43dad39d32f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7e16e73-e3a1-4f6c-8978-1faa16ed1641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LLM model\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo-0125', temperature=0)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        'context': itemgetter('query') | retriever, # here we send the query to the retriever to extract the most relevant chunks (or documents)\n",
    "        'question': itemgetter('query')\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "638f742e-7cc0-4203-b8b2-9061115f53a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chain.invoke({'query': \"Who are the judges?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "634cbc14-97ce-4f32-9b5c-6e23c739e579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The judges in the case of xxxxxxxxxxxxx were Lado Chanturia, Mykola Gnatovskyy, and Úna Ní Raifeartaigh. Judge Ch. was also mentioned in the context as having examined the case and delivered a verdict. Judge Ch. was later dismissed by a presidential order.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = res[:25] + ' xxxxxxxxxxxxx ' +res[43:]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae50b7-c84f-4782-a2a3-81d0184c179e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fccdd65-c408-48db-86ab-00f93449be0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-langchain",
   "language": "python",
   "name": "llm-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
